{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8470273,"sourceType":"datasetVersion","datasetId":5050563}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Common Imports","metadata":{}},{"cell_type":"code","source":"! if [ ! $pip_done ]; then pip install -q transformers ;fi \n! if [ ! $pip_done ]; then pip install -q datasets jiwer ;fi \n! if [ ! $pip_done ]; then pip install -q sentencepiece ;fi \n\npip_done = 1","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:07:59.196941Z","iopub.execute_input":"2024-05-20T20:07:59.197496Z","iopub.status.idle":"2024-05-20T20:08:38.298996Z","shell.execute_reply.started":"2024-05-20T20:07:59.197451Z","shell.execute_reply":"2024-05-20T20:08:38.297783Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel, AdamW\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:38.300937Z","iopub.execute_input":"2024-05-20T20:08:38.301268Z","iopub.status.idle":"2024-05-20T20:08:45.727901Z","shell.execute_reply.started":"2024-05-20T20:08:38.301237Z","shell.execute_reply":"2024-05-20T20:08:45.727123Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"root_dir = \"/kaggle/input/str-arabic-dataset/Arabic_words_train\"","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:45.729061Z","iopub.execute_input":"2024-05-20T20:08:45.729609Z","iopub.status.idle":"2024-05-20T20:08:45.733766Z","shell.execute_reply.started":"2024-05-20T20:08:45.729571Z","shell.execute_reply":"2024-05-20T20:08:45.732885Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"column_names = ['image_path', 'text']\ndf = pd.read_csv(\"/kaggle/input/str-arabic-dataset/Arabic_words_train/gt.txt\",names = column_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:45.735888Z","iopub.execute_input":"2024-05-20T20:08:45.736492Z","iopub.status.idle":"2024-05-20T20:08:45.764632Z","shell.execute_reply.started":"2024-05-20T20:08:45.736459Z","shell.execute_reply":"2024-05-20T20:08:45.763716Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test_size = 0.2\ntrain_df, test_df = train_test_split(df, test_size=test_size, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:45.765782Z","iopub.execute_input":"2024-05-20T20:08:45.766068Z","iopub.status.idle":"2024-05-20T20:08:45.777724Z","shell.execute_reply.started":"2024-05-20T20:08:45.766044Z","shell.execute_reply":"2024-05-20T20:08:45.776727Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:49.150837Z","iopub.execute_input":"2024-05-20T20:08:49.151593Z","iopub.status.idle":"2024-05-20T20:08:49.156583Z","shell.execute_reply.started":"2024-05-20T20:08:49.151561Z","shell.execute_reply":"2024-05-20T20:08:49.155460Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:50.135774Z","iopub.execute_input":"2024-05-20T20:08:50.136169Z","iopub.status.idle":"2024-05-20T20:08:50.151527Z","shell.execute_reply.started":"2024-05-20T20:08:50.136134Z","shell.execute_reply":"2024-05-20T20:08:50.150754Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      image_path   text\n0  word_1902.png  إبداع\n1  word_2924.png   مقتل\n2  word_3297.png   عروض\n3  word_2206.png   نوجا\n4  word_2274.png  فعالة","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>word_1902.png</td>\n      <td>إبداع</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>word_2924.png</td>\n      <td>مقتل</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>word_3297.png</td>\n      <td>عروض</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>word_2206.png</td>\n      <td>نوجا</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>word_2274.png</td>\n      <td>فعالة</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:50.880303Z","iopub.execute_input":"2024-05-20T20:08:50.881154Z","iopub.status.idle":"2024-05-20T20:08:50.889713Z","shell.execute_reply.started":"2024-05-20T20:08:50.881121Z","shell.execute_reply":"2024-05-20T20:08:50.888826Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"      image_path     text\n0  word_1743.png       فى\n1  word_3814.png      يوم\n2  word_1515.png      على\n3    word_96.png  فستيفال\n4  word_4073.png   الخاصة","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>word_1743.png</td>\n      <td>فى</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>word_3814.png</td>\n      <td>يوم</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>word_1515.png</td>\n      <td>على</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>word_96.png</td>\n      <td>فستيفال</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>word_4073.png</td>\n      <td>الخاصة</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:51.514517Z","iopub.execute_input":"2024-05-20T20:08:51.515153Z","iopub.status.idle":"2024-05-20T20:08:51.521063Z","shell.execute_reply.started":"2024-05-20T20:08:51.515119Z","shell.execute_reply":"2024-05-20T20:08:51.520150Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((3350, 2), (838, 2))"},"metadata":{}}]},{"cell_type":"code","source":"class ArabicSTRDataset(Dataset):\n    def __init__(self, root_dir, df, processor, tokenizer, max_target_length):\n        self.root_dir = root_dir\n        self.df = df\n        self.processor = processor\n        self.tokenizer = tokenizer\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Get file name and text\n        file_name = self.df.iloc[idx]['image_path']\n        text = self.df.iloc[idx]['text']\n\n        # Prepare image (resize and normalize)\n        image_path = f\"{self.root_dir}/{file_name}\"\n        image = Image.open(image_path).convert(\"RGB\")\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n\n        # Encode the text\n        labels = self.tokenizer(text, padding=\"max_length\", max_length=self.max_target_length, return_tensors=\"pt\").input_ids\n        labels = labels.squeeze()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": labels}\n        return encoding","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:52.015068Z","iopub.execute_input":"2024-05-20T20:08:52.015448Z","iopub.status.idle":"2024-05-20T20:08:52.024895Z","shell.execute_reply.started":"2024-05-20T20:08:52.015417Z","shell.execute_reply":"2024-05-20T20:08:52.023992Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:53.880790Z","iopub.execute_input":"2024-05-20T20:08:53.881495Z","iopub.status.idle":"2024-05-20T20:08:53.907206Z","shell.execute_reply.started":"2024-05-20T20:08:53.881462Z","shell.execute_reply":"2024-05-20T20:08:53.906241Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:54.663446Z","iopub.execute_input":"2024-05-20T20:08:54.663779Z","iopub.status.idle":"2024-05-20T20:08:54.669437Z","shell.execute_reply.started":"2024-05-20T20:08:54.663755Z","shell.execute_reply":"2024-05-20T20:08:54.668500Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-stage1\")\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-stage1')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:08:56.167213Z","iopub.execute_input":"2024-05-20T20:08:56.167574Z","iopub.status.idle":"2024-05-20T20:09:11.694479Z","shell.execute_reply.started":"2024-05-20T20:08:56.167538Z","shell.execute_reply":"2024-05-20T20:09:11.693595Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff4372607304e44ab069dc49406f4bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"289a0cdf91b748e4b0c8544b7c203e3b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"075916a7c47b41b8bb35ab8e417fdcbd"}},"metadata":{}},{"name":"stderr","text":"2024-05-20 20:09:01.553244: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-20 20:09:01.553370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-20 20:09:01.693662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"195810b0ee1b4e168c4c5701b13110f1"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b3f1dff6064cf7b0286fe9d1c03fa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"858406a95af942ec9f124035bd6b247c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed8e08c849ca46d8908e6452d20eb0ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde590b4079f4914a9cc259dfee62518"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): DeiTModel(\n    (embeddings): DeiTEmbeddings(\n      (patch_embeddings): DeiTPatchEmbeddings(\n        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): DeiTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x DeiTLayer(\n          (attention): DeiTAttention(\n            (attention): DeiTSelfAttention(\n              (query): Linear(in_features=384, out_features=384, bias=True)\n              (key): Linear(in_features=384, out_features=384, bias=True)\n              (value): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): DeiTSelfOutput(\n              (dense): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): DeiTIntermediate(\n            (dense): Linear(in_features=384, out_features=1536, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DeiTOutput(\n            (dense): Linear(in_features=1536, out_features=384, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n    (pooler): DeiTPooler(\n      (dense): Linear(in_features=384, out_features=384, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-5): 6 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 512\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:09:11.696902Z","iopub.execute_input":"2024-05-20T20:09:11.697223Z","iopub.status.idle":"2024-05-20T20:09:11.703479Z","shell.execute_reply.started":"2024-05-20T20:09:11.697198Z","shell.execute_reply":"2024-05-20T20:09:11.701748Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_dataset = ArabicSTRDataset(root_dir=root_dir,\n                           df=train_df,\n                           processor=processor,\n                           tokenizer=processor.tokenizer,\n                           max_target_length=100)\n\neval_dataset = ArabicSTRDataset(root_dir=root_dir,\n                           df=test_df,\n                           processor=processor,\n                           tokenizer=processor.tokenizer,\n                           max_target_length=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:09:35.403444Z","iopub.execute_input":"2024-05-20T20:09:35.404062Z","iopub.status.idle":"2024-05-20T20:09:35.409238Z","shell.execute_reply.started":"2024-05-20T20:09:35.404032Z","shell.execute_reply":"2024-05-20T20:09:35.408224Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"batch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:09:36.318505Z","iopub.execute_input":"2024-05-20T20:09:36.318940Z","iopub.status.idle":"2024-05-20T20:09:36.323131Z","shell.execute_reply.started":"2024-05-20T20:09:36.318895Z","shell.execute_reply":"2024-05-20T20:09:36.322157Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\neval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:09:36.638399Z","iopub.execute_input":"2024-05-20T20:09:36.638837Z","iopub.status.idle":"2024-05-20T20:09:36.644307Z","shell.execute_reply.started":"2024-05-20T20:09:36.638794Z","shell.execute_reply":"2024-05-20T20:09:36.643199Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:09:38.214390Z","iopub.execute_input":"2024-05-20T20:09:38.214780Z","iopub.status.idle":"2024-05-20T20:09:38.225646Z","shell.execute_reply.started":"2024-05-20T20:09:38.214745Z","shell.execute_reply":"2024-05-20T20:09:38.224766Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def train(EPOCHS, version, model, train_dataloader, optimizer, device):\n    \n    hist = []\n    for epoch in range(EPOCHS):  \n       # train\n       model.train()\n       train_loss = 0.0\n       for i, batch in enumerate(tqdm(train_dataloader)):\n          # get the inputs\n          for k,v in batch.items():\n            batch[k] = v.to(device)\n\n          # forward + backward + optimize\n          outputs = model(**batch)\n          loss = outputs.loss\n          loss.backward()\n          optimizer.step()\n          optimizer.zero_grad()\n\n          train_loss += loss.item()\n          #if i % 100 == 0: print(f\"Loss: {loss.item()}\") \n\n       print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n       hist.append(train_loss/len(train_dataloader))\n       model.save_pretrained(f\"version_{version}/epoch_{epoch}\")\n    \n    model.save_pretrained(f\"version_{version}/final\")\n    return hist","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:09:39.867229Z","iopub.execute_input":"2024-05-20T20:09:39.867592Z","iopub.status.idle":"2024-05-20T20:09:39.875204Z","shell.execute_reply.started":"2024-05-20T20:09:39.867563Z","shell.execute_reply":"2024-05-20T20:09:39.874286Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train(20, \"model0\", model, train_dataloader, optimizer, device)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:09:41.220902Z","iopub.execute_input":"2024-05-20T20:09:41.221570Z","iopub.status.idle":"2024-05-20T20:52:16.069067Z","shell.execute_reply.started":"2024-05-20T20:09:41.221529Z","shell.execute_reply":"2024-05-20T20:52:16.068237Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa63c962c6ae4a89aa1f2e4c19721105"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 0: 3.2472673105453818\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3220c7ca91448cea009800351a39442"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 1: 2.115452472520614\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7cfd7ea5e6463e90ac18cec2e20ec4"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 2: 1.9365794698492156\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6cb978fe5b94363afedab2785a5aa59"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 3: 1.6834036689385026\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a174318255aa4661901f205f3355361d"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 4: 1.3960979916599883\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa79e27e863d4a95b6b91b16b23aa502"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 5: 1.0780897122721114\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa61e656291e49908056c7ef6c43a9ee"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 6: 0.8039229431840855\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eeb493658a243a29673ad8c2991530d"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 7: 0.5946513017492818\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0586388cf84a45ccb70b503d067ac69f"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 8: 0.4577342018947396\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae43d347ba93442f96255dd549b183ff"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 9: 0.35260590479257997\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb149140b154610b605fdc80cd474bd"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 10: 0.27447615752801263\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28c268f433e4048b4a4d527bb246126"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 11: 0.2216197220355201\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12ed85fc017042ac84c577288e745d4c"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 12: 0.19348403612298443\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"356e64f4033b4833a5fbf10e03eae9ac"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 13: 0.18736610980148746\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81c8e946738d41c4876a8df35a307b69"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 14: 0.151078425470055\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f3e45f5155f4e6bab3473425a3e8b5b"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 15: 0.143357198674294\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc594a225544966a57bbcc50983b229"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 16: 0.11924972611997356\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2ec1ef441cc4e1c8b789f74b99d9159"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 17: 0.11558081013281654\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee1d6e499744912abcd1bd544eb781e"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 18: 0.10059404833440119\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04237072274747ff9ce500d6061fdce9"}},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"name":"stdout","text":"Loss after epoch 19: 0.0914248395366661\n","output_type":"stream"},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def generate_text(image_path):\n    # Read and process the image\n    image = Image.open(image_path).convert(\"RGB\")\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n\n    # Generate text\n    outputs = model.generate(pixel_values, num_beams=4, max_length=512, early_stopping=True)\n    predicted_text = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return predicted_text","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:53:51.329901Z","iopub.execute_input":"2024-05-20T20:53:51.330303Z","iopub.status.idle":"2024-05-20T20:53:51.336168Z","shell.execute_reply.started":"2024-05-20T20:53:51.330271Z","shell.execute_reply":"2024-05-20T20:53:51.335249Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"generate_text(\"/kaggle/input/str-arabic-dataset/Arabic_words_train/word_1.png\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:54:06.129658Z","iopub.execute_input":"2024-05-20T20:54:06.130334Z","iopub.status.idle":"2024-05-20T20:54:06.535162Z","shell.execute_reply.started":"2024-05-20T20:54:06.130300Z","shell.execute_reply":"2024-05-20T20:54:06.534216Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'خاص'"},"metadata":{}}]},{"cell_type":"code","source":"generate_text(\"/kaggle/input/str-arabic-dataset/Arabic_words_train/word_1008.png\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:55:55.747156Z","iopub.execute_input":"2024-05-20T20:55:55.747906Z","iopub.status.idle":"2024-05-20T20:55:55.858436Z","shell.execute_reply.started":"2024-05-20T20:55:55.747874Z","shell.execute_reply":"2024-05-20T20:55:55.857650Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'هيوصل'"},"metadata":{}}]},{"cell_type":"code","source":"generate_text(\"/kaggle/input/str-arabic-dataset/Arabic_words_train/word_101.png\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:56:38.875214Z","iopub.execute_input":"2024-05-20T20:56:38.875888Z","iopub.status.idle":"2024-05-20T20:56:39.007501Z","shell.execute_reply.started":"2024-05-20T20:56:38.875852Z","shell.execute_reply":"2024-05-20T20:56:39.006642Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'إلحقي'"},"metadata":{}}]},{"cell_type":"code","source":"generate_text(\"/kaggle/input/str-arabic-dataset/Arabic_words_train/word_1076.png\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:57:23.202897Z","iopub.execute_input":"2024-05-20T20:57:23.203792Z","iopub.status.idle":"2024-05-20T20:57:23.322836Z","shell.execute_reply.started":"2024-05-20T20:57:23.203758Z","shell.execute_reply":"2024-05-20T20:57:23.321909Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'شركة'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}